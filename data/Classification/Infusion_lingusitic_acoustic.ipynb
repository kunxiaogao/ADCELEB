{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix,roc_auc_score\n",
    "\n",
    "before_5_non_acoustic =  ''\n",
    "before_5_non_linguistic =''\n",
    "\n",
    "before_10_non_acoustic =  ''\n",
    "before_10_non_linguistic =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={before_5_non_acoustic : ['wav2vec','whisper','hubert'], # Replace with actual top3 embedding names \n",
    "before_10_non_acoustic : ['wav2vec','hubert','xvector'], # Replace with actual top3 embedding names \n",
    "before_5_non_linguistic : ['lama3','e5-large','cross-en-fr-roberta'], # Replace with actual top3 embedding names \n",
    "before_10_non_linguistic : ['cross-en-fr-roberta','distiluse-v1','e5-large']} # Replace with actual top3 embedding names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.read_excel('speakes_pairs.xlsx')\n",
    "\n",
    "# Extract and format names, replacing spaces with underscores\n",
    "cn_names = [elem.replace(\" \", \"_\") for elem in df_stats['Name'].tolist()]\n",
    "pd_names = [elem.split(\"\\t\")[0].replace(\" \", \"_\") for elem in df_stats['Use as a control for'].tolist()]\n",
    "\n",
    "# Assign labels: 1 for control names (cn_names), 0 for patient names (pd_names)\n",
    "label_dict = {name: 1 for name in cn_names}\n",
    "label_dict.update({name: 0 for name in pd_names})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def combine_speaker_label(files):\n",
    "    \"\"\"\n",
    "    Combines the 'speaker' and 'label' columns from multiple files into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - files (list of str): List of file paths.\n",
    "    \n",
    "    Returns:\n",
    "    - combined_df (pd.DataFrame): DataFrame with combined 'speaker' and 'label' data.\n",
    "    \"\"\"\n",
    "    combined_df = pd.DataFrame(columns=['Speaker', 'Predicted_Label'])\n",
    "    \n",
    "    for file in files:\n",
    "        # Load each file\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Check if 'speaker' and 'label' columns exist\n",
    "        if 'Speaker' in df.columns and 'Predicted_Label' in df.columns:\n",
    "            # Extract the 'speaker' and 'label' columns and append to combined_df\n",
    "            combined_df = pd.concat([combined_df, df[['Speaker', 'Predicted_Label']]], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"File {file} does not contain 'speaker' or 'label' columns and was skipped.\")\n",
    "\n",
    "    return combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_and_calculate_metrics(files, label_dict):\n",
    "    \"\"\"\n",
    "    Combines 'Speaker' and 'Predicted_Label' columns from multiple files, groups by 'Speaker' to calculate\n",
    "    the average of 'Predicted_Label', applies a threshold, maps true labels, and calculates metrics.\n",
    "    \n",
    "    Args:\n",
    "    - files (list of str): List of file paths.\n",
    "    - label_dict (dict): Dictionary mapping speakers to true labels.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with 'Speaker', thresholded 'Predicted_Label', and 'True_Label'.\n",
    "    - dict: Dictionary containing accuracy, F1 score, sensitivity, and specificity.\n",
    "    \"\"\"\n",
    "        # Combine data from files\n",
    "    combined_df = combine_speaker_label(files)\n",
    "\n",
    "    # Group by 'Speaker' and calculate the mean of 'Predicted_Label'\n",
    "    combined_df = combined_df.groupby('Speaker', as_index=False).mean()\n",
    "\n",
    "    raw_predictions = combined_df['Predicted_Label'].copy()\n",
    "\n",
    "    # Apply threshold to 'Predicted_Label': >= 0.5 becomes 1, < 0.5 becomes 0\n",
    "    combined_df['Predicted_Label'] = combined_df['Predicted_Label'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "\n",
    "    # Map true labels to a new column 'True_Label' using label_dict\n",
    "    combined_df['True_Label'] = combined_df['Speaker'].map(label_dict)\n",
    "\n",
    "    # Step 3: Calculate metrics\n",
    "    valid_rows = combined_df['True_Label'].notna()  # Filter out rows without true labels\n",
    "    y_true = combined_df.loc[valid_rows, 'True_Label']\n",
    "    y_pred = combined_df.loc[valid_rows, 'Predicted_Label']\n",
    "    y_scores = raw_predictions[valid_rows]  # Use raw predictions for AUC\n",
    "\n",
    "    # Calculate metrics using sklearn\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred)  # Sensitivity is the same as recall\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else None\n",
    "    auc = roc_auc_score(y_true, y_scores)  # Calculate AUC with raw scores\n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'AUC': auc\n",
    "    }\n",
    "\n",
    "    return combined_df, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8,\n",
       " 'f1_score': 0.8125,\n",
       " 'sensitivity': 0.8666666666666667,\n",
       " 'specificity': 0.7333333333333333,\n",
       " 'AUC': 0.8577777777777778}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [before_5_non_acoustic,before_5_non_linguistic]  # Replace with actual file paths\n",
    "combined_df, metrics = generate_table_and_calculate_metrics(files,label_dict)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7166666666666667,\n",
       " 'f1_score': 0.7384615384615385,\n",
       " 'sensitivity': 0.8,\n",
       " 'specificity': 0.6333333333333333,\n",
       " 'AUC': 0.7744444444444445}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [before_10_non_acoustic,before_10_non_linguistic]  # Replace with actual file paths\n",
    "combined_df, metrics = generate_table_and_calculate_metrics(files,label_dict)\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
